{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import pixiedust\n",
    "from gym.envs.registration import register\n",
    "from qiskit.aqua.components.optimizers import ADAM\n",
    "from qiskit.aqua.components.variational_forms.ry import RY\n",
    "from qiskit import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AccountProvider for IBMQ(hub='ibm-q', group='open', project='main')>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IBMQ.load_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.8196, # optimum = .8196, changing this seems have no influence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLakeNotSlippery-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numStates(env):\n",
    "    if type(env.observation_space) != gym.spaces.tuple.Tuple:\n",
    "        return env.observation_space.n\n",
    "    dim_list = []\n",
    "    for sp in env.observation_space:\n",
    "        dim_list.append(sp.n)\n",
    "    dim_list = np.array(dim_list)\n",
    "    return dim_list.prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Statehash(env,state):\n",
    "    if type(env.observation_space) != gym.spaces.tuple.Tuple:\n",
    "        return state\n",
    "    dim_list = []\n",
    "    for sp in env.observation_space:\n",
    "        dim_list.append(sp.n)\n",
    "    dim_list = np.array(dim_list)\n",
    "    h = 0\n",
    "    for i in range(len(dim_list)-1):\n",
    "        h += state[i]*dim_list[i+1:].prod()\n",
    "    h += state[-1]\n",
    "    return h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(iterations,alpha, gamma, epsilon ,env):\n",
    "    returnlist = []\n",
    "    num_actions = env.action_space.n\n",
    "    num_states = get_numStates(env)\n",
    "    Q = np.zeros((num_states,num_actions))\n",
    "    for it in range(iterations):\n",
    "        state = env.reset()\n",
    "        R = 0\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_h = get_Statehash(env,state)\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                candidates = np.where(Q[state_h] == np.max(Q[state_h]))[0]\n",
    "                action = np.random.choice(candidates)\n",
    "            statep, reward, done, info = env.step(action)\n",
    "            if reward == 0:\n",
    "                if done:\n",
    "                    reward = -0.2\n",
    "                else:\n",
    "                    reward = -0.01\n",
    "            else:\n",
    "                reward = 1.0\n",
    "            R *= gamma\n",
    "            R += reward\n",
    "            statep_h = get_Statehash(env,statep)\n",
    "            Q[state_h,action] += alpha*(reward + gamma*Q[statep_h].max() -Q[state_h,action])\n",
    "            state = statep\n",
    "            \n",
    "        returnlist.append(R)\n",
    "        if it%10000 == 0:\n",
    "            print('Iteration %d, Reward: %d'%(it,R))\n",
    "            \n",
    "    return returnlist, Q\n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl, Q = QLearning(100000,1e-3,0.99,0.25,env)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     backend_sim = Aer.get_backend('qasm_simulator')\n",
    "\n",
    "#     result = execute(qc, backend_sim, shots = shots).result()\n",
    "\n",
    "#     counts = result.get_counts()\n",
    "#     expect = np.zeros(nqbits)\n",
    "#     for c in counts :\n",
    "#         for n in range(nqbits):\n",
    "#             expect[n] += int(c[n])*counts[c]/shots\n",
    "#     #print(counts)\n",
    "#     return np.array(expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Qvalues(s_list,theta):\n",
    "    shots = 1000\n",
    "    qc_list = []\n",
    "    nqbits = 4\n",
    "    theta.flatten()\n",
    "    for s in s_list:\n",
    "        q = QuantumRegister(nqbits)\n",
    "        c = ClassicalRegister(nqbits)\n",
    "        qc = QuantumCircuit(q,c)\n",
    "\n",
    "        d = np.binary_repr(int(s),nqbits)\n",
    "        for j,i in enumerate(d):\n",
    "            if i == '1':\n",
    "                qc.x(q[nqbits - j - 1]) #changed\n",
    "        \n",
    "        qc += RY(nqbits,1, [[0,1],[1,2],[2,3]], 'linear', None, 'cx').construct_circuit(theta,q)        \n",
    "        qc_list.append(qc)\n",
    "\n",
    "\n",
    "    \n",
    "    backend_sim = Aer.get_backend('statevector_simulator')\n",
    "    qobj = assemble(qc_list,backend_sim)\n",
    "    #result_list = execute(qc_list, backend_sim, optimization_level = 0).result()\n",
    "    job = backend_sim.run(qobj)\n",
    "    result_list = job.result()\n",
    "    expect_list = []\n",
    "    for result in result_list.results:\n",
    "        proba = abs(np.array(result.data.statevector))**2\n",
    "\n",
    "\n",
    "        expect = np.zeros(nqbits)\n",
    "\n",
    "        for c in range(len(proba)):\n",
    "            cbin = np.binary_repr(int(c),nqbits)\n",
    "\n",
    "            for n in range(nqbits) : \n",
    "                if cbin[nqbits - n - 1] == '1': #changed\n",
    "                    expect[n] += proba[c]\n",
    "                    \n",
    "        expect_list.append(expect) #changed\n",
    "                \n",
    "    #print(counts)\n",
    "    return expect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(theta, t):\n",
    "    su = 0\n",
    "    Qs = get_Qvalues(t[:,1],theta)\n",
    "    \n",
    "    for i in range(len(t)):\n",
    "        su += (t[i][0] - Qs[i][int(t[i][2])])**2\n",
    "    return su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN(iterations,alpha, gamma, epsilon ,env,N,batchsize):\n",
    "    nqbits = 4\n",
    "    D = []\n",
    "    while len(D) < N:\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = env.action_space.sample()\n",
    "            s1, r, done,_ = env.step(a)\n",
    "            if r == 0:\n",
    "                if done:\n",
    "                    r = -0.2\n",
    "                else:\n",
    "                    r = -0.01\n",
    "            else:\n",
    "                r = 1.0\n",
    "            D.append((s,a,r,s1,done))\n",
    "            s = s1\n",
    "            if len(D) == N:\n",
    "                break\n",
    "    total_rewards = []\n",
    "    theta = np.array([2*np.pi*np.random.random(2),2*np.pi*np.random.random(2),2*np.pi*np.random.random(2),2*np.pi*np.random.random(2)]).flatten()\n",
    "    for it in range(iterations):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            if np.random.random() < epsilon*(0.99**it):\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = np.argmax(get_Qvalues([s],theta)[0])\n",
    "            s1,r,done,_ = env.step(a)\n",
    "            if r == 0:\n",
    "                if done:\n",
    "                    r = -0.2\n",
    "                else:\n",
    "                    r = -0.01\n",
    "            else:\n",
    "                r = 1.0\n",
    "            #print(r)\n",
    "            total_reward += r\n",
    "            s = s1\n",
    "            D.pop(0)\n",
    "            D.append((s,a,r,s1,done))\n",
    "        mB_ind = np.random.choice(range(N),size = batchsize,replace = False)\n",
    "        mB = np.array(D)[mB_ind]\n",
    "        #update Q on mB\n",
    "        t = []\n",
    "        for j in range(batchsize):\n",
    "            if mB[j][-1]:\n",
    "                y_j = mB[j][2]\n",
    "            else:\n",
    "                y_j = mB[j][2] + gamma*(2*max(get_Qvalues([mB[j][3]],theta)[0])-1)\n",
    "            y_j /= 2\n",
    "            y_j += 0.5\n",
    "            t.append([y_j,mB[j][0],mB[j][1]])\n",
    "\n",
    "        t = np.array(t)\n",
    "\n",
    "        adam = ADAM(maxiter = 10, lr = alpha)\n",
    "        start = datetime.now()\n",
    "        print(theta)\n",
    "        theta,_,_ = adam.optimize(2*nqbits,lambda x: loss(x,t), initial_point = theta)\n",
    "        print(datetime.now()-start)\n",
    "        if it %1 == 0:\n",
    "            print('Iteration : ', it, 'Total reward: ', total_reward)\n",
    "        total_rewards.append(total_reward)\n",
    "        \n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mmoll\\anaconda3\\envs\\quantum\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Insufficient bit width provided. This behavior will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.84927497 1.14986342 3.54230691 6.18838833 0.47717541 0.05057074\n",
      " 5.60634159 2.24829141]\n",
      "0:00:18.199401\n",
      "Iteration :  0 Total reward:  -0.3\n",
      "[ 5.87111368  2.13795325  4.28703621  7.09273692  1.41072874 -0.9041351\n",
      "  4.88065385  1.91895234]\n",
      "0:00:14.460910\n",
      "Iteration :  1 Total reward:  -0.3\n",
      "[ 6.60866871  2.67624726  4.85991204  7.89027748  2.15594666 -0.51780066\n",
      "  4.38572373  1.18437063]\n",
      "0:00:09.740103\n",
      "Iteration :  2 Total reward:  -0.27\n",
      "[ 6.3693413   2.97792161  4.627046    7.93893425  2.0935005  -0.23874476\n",
      "  4.72314689  1.21801426]\n",
      "0:00:09.843921\n",
      "Iteration :  3 Total reward:  -0.32\n",
      "[ 7.01630895  2.02484246  4.31866509  7.80788782  2.82180655 -0.29014308\n",
      "  5.02293523  1.80842972]\n",
      "0:00:09.458441\n",
      "Iteration :  4 Total reward:  -0.24000000000000002\n",
      "[ 6.39863114  2.5844638   4.90843465  7.93371349  2.82170724 -0.91635717\n",
      "  4.50885008  1.55067347]\n",
      "0:00:09.424773\n",
      "Iteration :  5 Total reward:  -0.37\n",
      "[ 6.28935226  2.53075192  5.37382173  8.01164945  3.19535886 -0.31361686\n",
      "  4.01286654  1.39327465]\n",
      "0:00:09.583565\n",
      "Iteration :  6 Total reward:  -0.23\n",
      "[ 6.35872097  2.13998002  4.65856428  7.68698864  2.90605615 -0.14898903\n",
      "  4.71441822  1.47058308]\n",
      "0:00:12.949730\n",
      "Iteration :  7 Total reward:  -0.3\n",
      "[ 6.00736185  2.78970232  4.84843481  7.63519657  2.41976541 -0.78786977\n",
      "  4.80851165  1.49313227]\n",
      "0:00:11.685486\n",
      "Iteration :  8 Total reward:  -0.27\n",
      "[ 6.60841608  2.69859666  4.80025944  8.07859355  3.09143891 -0.35304132\n",
      "  4.67071439  1.58605957]\n",
      "0:00:10.876240\n",
      "Iteration :  9 Total reward:  -0.3\n",
      "[ 6.16387184  2.62119357  4.87105849  7.67192371  3.15263182 -0.56082696\n",
      "  4.01308694  1.55348818]\n",
      "0:00:11.610941\n",
      "Iteration :  10 Total reward:  -0.25\n",
      "[ 5.96345971  2.89566084  4.25181546  7.89093959  3.16949622 -0.86787874\n",
      "  4.61187305  1.44328119]\n",
      "0:00:19.229882\n",
      "Iteration :  11 Total reward:  -0.36\n",
      "[ 5.7159385   3.74870154  3.81163988  7.85657614  2.69929383 -1.69462903\n",
      "  4.99940708  1.56957425]\n",
      "0:00:14.321569\n",
      "Iteration :  12 Total reward:  -0.33999999999999997\n",
      "[ 6.034162    2.92430016  3.35779273  8.24096319  3.26158711 -0.81738826\n",
      "  4.08831597  1.27037229]\n",
      "0:00:15.409826\n",
      "Iteration :  13 Total reward:  -0.39\n",
      "[5.99561641 3.36762231 3.99254962 8.1834801  4.2408418  0.03976793\n",
      " 3.56742913 1.1407697 ]\n",
      "0:00:14.896174\n",
      "Iteration :  14 Total reward:  -0.29000000000000004\n",
      "[6.42264073 3.06890127 3.64301675 7.73774475 3.77525807 0.06834094\n",
      " 3.90927859 1.63785231]\n",
      "0:00:13.378874\n",
      "Iteration :  15 Total reward:  -0.22\n",
      "[6.38814217 2.51688361 4.15681843 7.93147031 2.89508581 0.70086433\n",
      " 4.35542534 1.67870473]\n",
      "0:00:09.899398\n",
      "Iteration :  16 Total reward:  -0.23\n",
      "[6.28061985e+00 3.21102399e+00 3.47395551e+00 7.34264071e+00\n",
      " 3.05916696e+00 3.43249715e-03 3.58783697e+00 2.09779796e+00]\n",
      "0:00:10.349798\n",
      "Iteration :  17 Total reward:  -0.21000000000000002\n",
      "[ 6.0419536   3.4145638   2.79733599  6.69664838  2.98018481 -0.2109033\n",
      "  4.2814441   2.45589542]\n",
      "0:00:09.701954\n",
      "Iteration :  18 Total reward:  -0.29000000000000004\n",
      "[ 6.27443492  3.71698967  3.34609887  6.38015194  3.16791867 -0.60768548\n",
      "  3.48997346  3.21109637]\n",
      "0:00:09.407711\n",
      "Iteration :  19 Total reward:  -0.24000000000000002\n",
      "[ 6.11548264  3.05065518  2.93764854  6.31626707  3.19247394 -0.21150304\n",
      "  3.07212921  3.10712809]\n",
      "0:00:09.060614\n",
      "Iteration :  20 Total reward:  -0.32\n",
      "[5.34229859 2.95814019 2.75308985 5.8070165  2.66473904 0.15180656\n",
      " 2.24599246 3.82934754]\n",
      "0:00:09.008166\n",
      "Iteration :  21 Total reward:  -0.43000000000000005\n",
      "[ 5.90390971  3.67665052  3.41045665  6.48111049  3.25518989 -0.27192857\n",
      "  2.96692571  3.21269174]\n",
      "0:00:13.331716\n",
      "Iteration :  22 Total reward:  -0.27\n",
      "[5.88400211 3.20079978 2.94934992 6.20548801 3.11490597 0.25232008\n",
      " 3.24993204 2.95718484]\n",
      "0:00:11.478691\n",
      "Iteration :  23 Total reward:  -0.32\n",
      "[6.12926818 2.90385792 3.22804497 6.40937547 2.8188098  0.31137046\n",
      " 3.02220625 3.20561573]\n",
      "0:00:15.705078\n",
      "Iteration :  24 Total reward:  -0.28\n",
      "[6.09633131 3.00588005 3.33246331 7.29314883 1.85171822 1.24311459\n",
      " 3.5351513  2.28622445]\n",
      "0:00:16.067611\n",
      "Iteration :  25 Total reward:  -0.32\n",
      "[6.89349862 2.48342383 2.61357789 7.71831315 2.67493262 2.16940124\n",
      " 4.45882041 1.75764791]\n",
      "0:00:22.367302\n",
      "Iteration :  26 Total reward:  -0.27\n",
      "[6.07407564 3.30855935 3.43013997 6.90364162 3.16605178 2.25594527\n",
      " 3.59745121 2.49514466]\n",
      "0:00:20.574147\n",
      "Iteration :  27 Total reward:  -0.32999999999999996\n",
      "[6.36855089 3.39749666 3.31419245 6.93652423 3.24037744 1.87518024\n",
      " 3.51587803 2.26914939]\n",
      "0:00:18.021079\n",
      "Iteration :  28 Total reward:  -0.39\n",
      "[6.153719   3.9133032  4.07839859 6.02727586 2.9932229  1.01228293\n",
      " 2.99840834 1.4583314 ]\n",
      "0:00:15.175975\n",
      "Iteration :  29 Total reward:  -0.22\n",
      "[6.23551357 3.69748479 3.77177432 6.94122216 3.07313764 0.31576397\n",
      " 3.27750486 2.2419046 ]\n",
      "0:00:12.843818\n",
      "Iteration :  30 Total reward:  -0.21000000000000002\n",
      "[ 6.29888579  3.6891802   3.30392354  7.42459131  2.69475686 -0.53691539\n",
      "  3.21644063  2.54308806]\n",
      "0:00:09.582703\n",
      "Iteration :  31 Total reward:  -0.28\n",
      "[6.23059676 3.05922689 2.81593134 6.96740746 3.23868918 0.0906562\n",
      " 3.01378626 2.07899044]\n",
      "0:00:12.122090\n",
      "Iteration :  32 Total reward:  -0.27\n",
      "[6.0464553  2.50664916 3.33811616 7.53501991 3.57122309 0.66931467\n",
      " 3.3598414  1.92168154]\n",
      "0:00:12.310712\n",
      "Iteration :  33 Total reward:  -0.27\n",
      "[6.36123536 2.91830878 3.44490308 8.03446979 3.07442649 0.79106881\n",
      " 3.43804315 1.37908639]\n",
      "0:00:17.315788\n",
      "Iteration :  34 Total reward:  -0.21000000000000002\n",
      "[6.70743434 2.67477036 3.77047969 7.64818953 2.78606977 0.66176039\n",
      " 2.77707952 1.40873173]\n",
      "0:00:13.899890\n",
      "Iteration :  35 Total reward:  -0.25\n",
      "[ 5.95523885  3.40965373  3.8817492   7.52258243  3.27601336 -0.02183905\n",
      "  2.49680476  1.86374485]\n",
      "0:00:12.754899\n",
      "Iteration :  36 Total reward:  -0.22\n",
      "[6.19843959 3.2868089  4.01703679 7.99074559 2.95656775 0.45451045\n",
      " 2.34190788 1.33819457]\n",
      "0:00:20.042989\n",
      "Iteration :  37 Total reward:  -0.6500000000000002\n",
      "[6.34987395 3.42454187 4.77177402 7.71721728 3.29696378 0.19394834\n",
      " 2.53251393 1.69057038]\n",
      "0:00:13.271494\n",
      "Iteration :  38 Total reward:  -0.27\n",
      "[ 6.65031032  3.81359396  4.36573281  7.94461606  3.64894659 -0.15843923\n",
      "  2.72592395  1.3776267 ]\n",
      "0:00:11.728644\n",
      "Iteration :  39 Total reward:  -0.32999999999999996\n",
      "[6.00079794 3.34126182 4.07019096 7.96584119 2.89228515 0.4419973\n",
      " 3.02106212 1.58303467]\n",
      "0:00:10.680442\n",
      "Iteration :  40 Total reward:  -0.31\n",
      "[5.96047693 2.68652894 3.36093554 7.77200295 3.38649986 1.09097148\n",
      " 3.74683256 1.68112914]\n",
      "0:00:14.635907\n",
      "Iteration :  41 Total reward:  -0.26\n",
      "[5.89557337 2.31450618 3.17377361 7.79124297 3.71723306 1.52249754\n",
      " 3.96554786 1.57486741]\n",
      "0:00:11.720059\n",
      "Iteration :  42 Total reward:  -0.5200000000000001\n",
      "[6.43626739 3.09796112 2.67954062 7.55065608 2.89461168 0.6040517\n",
      " 3.02555341 1.85250279]\n",
      "0:00:12.328280\n",
      "Iteration :  43 Total reward:  -0.6300000000000002\n",
      "[ 6.17512532  2.99213955  2.68510015  8.0621529   3.05815363 -0.03667717\n",
      "  2.67904024  1.23019659]\n",
      "0:00:13.638772\n",
      "Iteration :  44 Total reward:  -0.37\n",
      "[ 6.19654641  3.16663835  2.68145147  8.07306212  3.08175981 -0.3372431\n",
      "  2.75087611  1.23300215]\n",
      "0:00:16.793598\n",
      "Iteration :  45 Total reward:  -0.43000000000000005\n",
      "[6.29185702 3.23935193 2.91757039 8.19163926 3.28578826 0.12105348\n",
      " 2.76723697 1.42321706]\n",
      "0:00:17.923006\n",
      "Iteration :  46 Total reward:  -0.5100000000000001\n",
      "[ 6.09321892  3.0474219   2.72877145  8.88913842  2.98966471 -0.01647596\n",
      "  2.95107101  1.37532452]\n",
      "0:00:16.058330\n",
      "Iteration :  47 Total reward:  -0.22\n",
      "[ 5.39272729  2.22998366  2.05344341  7.93935294  2.06612939 -0.93530171\n",
      "  2.14454943  1.75430561]\n",
      "0:00:18.228728\n",
      "Iteration :  48 Total reward:  -0.35\n",
      "[ 6.2190918   3.0435846   2.48260576  7.96482838  2.89386742 -0.10023679\n",
      "  2.25063459  1.59663113]\n",
      "0:00:20.841111\n",
      "Iteration :  49 Total reward:  -0.36\n",
      "[6.38200505 3.30568956 2.25178769 7.98853296 3.30494093 0.1048973\n",
      " 2.44069812 1.59998545]\n",
      "0:00:20.506744\n",
      "Iteration :  50 Total reward:  -0.37\n",
      "[ 6.18746007  3.10596844  2.26296528  7.78998058  3.08018732 -0.09468518\n",
      "  2.43083011  1.44926255]\n",
      "0:00:18.749010\n",
      "Iteration :  51 Total reward:  -0.32\n",
      "[ 5.81908551  2.78882139  2.05920138  7.90585895  2.43083512 -0.42118432\n",
      "  2.35253763  1.69352388]\n",
      "0:00:10.915510\n",
      "Iteration :  52 Total reward:  -0.5000000000000001\n",
      "[6.4419916  3.38283865 2.61668646 7.91356898 3.10030085 0.15406468\n",
      " 2.96993944 1.50864397]\n",
      "0:00:14.705618\n",
      "Iteration :  53 Total reward:  -0.6000000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.16140621  3.04663121  2.86637034  8.2442972   3.23699321 -0.15263937\n",
      "  2.74666189  0.85918534]\n",
      "0:00:13.483129\n",
      "Iteration :  54 Total reward:  -0.23\n",
      "[ 6.24444912  3.22061461  2.81758004  8.40912667  3.24852848 -0.05054925\n",
      "  3.0742939   1.12283147]\n",
      "0:00:13.152826\n",
      "Iteration :  55 Total reward:  -0.27\n",
      "[6.07167529 3.2196529  3.09855167 8.49465971 3.10976333 0.05197543\n",
      " 3.04695314 0.95714017]\n",
      "0:00:10.562608\n",
      "Iteration :  56 Total reward:  -0.27\n",
      "[5.28360549 4.14212861 2.18315678 7.61969607 2.41568409 0.98299681\n",
      " 2.34025976 1.7782641 ]\n",
      "0:00:10.361359\n",
      "Iteration :  57 Total reward:  -0.6600000000000003\n",
      "[5.40190179 4.46277998 2.26064687 7.97698436 2.13480838 0.42936842\n",
      " 2.42610668 1.58952314]\n",
      "0:00:09.869981\n",
      "Iteration :  58 Total reward:  -0.23\n",
      "[ 6.10211121  3.79941716  1.60486521  8.20801225  3.0521088  -0.44174778\n",
      "  1.72364788  1.47658913]\n",
      "0:00:11.331196\n",
      "Iteration :  59 Total reward:  -0.43000000000000005\n",
      "[ 6.39693297  3.68271229  1.80430785  7.71493437  3.30448428 -0.58936548\n",
      "  1.56277835  1.60846373]\n",
      "0:00:13.192131\n",
      "Iteration :  60 Total reward:  -0.27\n",
      "[ 6.03946549  3.95042634  1.27797139  7.92500633  3.13964269 -1.11231775\n",
      "  2.05224803  1.98115486]\n",
      "0:00:13.104445\n",
      "Iteration :  61 Total reward:  -0.28\n",
      "[ 5.83216048  3.03927448  1.82392835  7.94433142  2.85894904 -2.03682515\n",
      "  1.47043183  1.3707502 ]\n",
      "0:00:11.230752\n",
      "Iteration :  62 Total reward:  -0.5300000000000001\n",
      "[ 6.38808176  3.55475002  1.55989197  7.73473081  3.35108831 -1.48678682\n",
      "  1.73942365  1.70533519]\n",
      "0:00:12.280627\n",
      "Iteration :  63 Total reward:  -0.7300000000000002\n",
      "[ 6.17145067  3.99547664  1.81439064  7.94340558  3.17209888 -1.09843298\n",
      "  1.51155816  1.36375959]\n",
      "0:00:12.446740\n",
      "Iteration :  64 Total reward:  -0.39\n",
      "[ 5.26682261  4.82346944  1.60889038  8.81313766  4.07196161 -1.13501462\n",
      "  1.66470561  1.09961456]\n",
      "0:00:12.729550\n",
      "Iteration :  65 Total reward:  -0.26\n",
      "[ 6.12964059  5.56395877  1.62157218  7.9467095   3.21165387 -2.08559923\n",
      "  1.86000468  1.7821783 ]\n"
     ]
    }
   ],
   "source": [
    "total_rewards = DQN(100,1e-1,0.99,0.9,env,100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_rewards,'x')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
